{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from entsoe import EntsoePandasClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_key = os.environ.get('ENTSOE_API_KEY')\n",
    "client = EntsoePandasClient(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = 'HU'\n",
    "years = ['2017', '2018', '2019', '2020', '2021', '2022', '2023']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_folder_path = './data/prices'\n",
    "loads_folder_path = './data/loads'\n",
    "\n",
    "base_df_filename = f'./data/base_{country_code}_{years[0]}_{years[-1]}.csv'\n",
    "base_load_df_filename = f'./data/base_load_avgs_{country_code}_{years[0]}_{years[-1]}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_querry_day_ahead_prices(country_code, start_date, end_date):\n",
    "    filename = f'price_{start_date}_{end_date}_{country_code}.csv'\n",
    "    start_ts = pd.Timestamp(start_date, tz='Europe/Budapest')\n",
    "    end_ts = pd.Timestamp(end_date, tz='Europe/Budapest')\n",
    "\n",
    "    if os.path.exists(f'{prices_folder_path}/{filename}'):\n",
    "        print(f'{prices_folder_path}/{filename} exists, reading from file')\n",
    "        #load\n",
    "        df = pd.read_csv(f'{prices_folder_path}/{filename}', index_col=0)\n",
    "    else:\n",
    "        print(f'{prices_folder_path}/{filename} does not exist, downloading from ENTSO-E')\n",
    "\n",
    "        #set start time to 00:00:00 and end time to 23:59:59\n",
    "        start_ts = pd.Timestamp(start_date, tz='Europe/Brussels')\n",
    "        end_ts = pd.Timestamp(end_date, tz='Europe/Brussels') + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n",
    "\n",
    "        df = client.query_day_ahead_prices(country_code, start=start_ts, end=end_ts)        # Data from ENTSO-E\n",
    "        \n",
    "        df.to_csv(f'{prices_folder_path}/{filename}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_querry_load(country_code, start_date, end_date):\n",
    "    filename = f'load_{start_date}_{end_date}_{country_code}.csv'\n",
    "    start_ts = pd.Timestamp(start_date, tz='Europe/Budapest')\n",
    "    end_ts = pd.Timestamp(end_date, tz='Europe/Budapest')\n",
    "\n",
    "    if os.path.exists(f'{loads_folder_path}/{filename}'):\n",
    "        print(f'{loads_folder_path}/{filename} exists, reading from file')\n",
    "        #load\n",
    "        df = pd.read_csv(f'{loads_folder_path}/{filename}', index_col=0)\n",
    "    else:\n",
    "        print(f'{loads_folder_path}/{filename} does not exist, downloading from ENTSO-E')\n",
    "\n",
    "        #set start time to 00:00:00 and end time to 23:59:59\n",
    "        start_ts = pd.Timestamp(start_date, tz='Europe/Brussels')\n",
    "        end_ts = pd.Timestamp(end_date, tz='Europe/Brussels') + pd.Timedelta(days=1)\n",
    "\n",
    "        df =client.query_load(country_code, start=start_ts, end=end_ts)        # Data from ENTSO-E\n",
    "        \n",
    "        df.to_csv(f'{loads_folder_path}/{filename}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the base concatenated df\n",
    "\n",
    "First we will create a base concatenated dataframe with all the data from the different files but only the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_prices(country_code, base_load_df_filename=base_load_df_filename, years=years):\n",
    "    if os.path.exists(base_df_filename):\n",
    "        print(f'{base_df_filename} exists, reading from file')\n",
    "        df = pd.read_csv(base_df_filename, index_col=0, parse_dates=True)\n",
    "    else:\n",
    "        print(f'{base_df_filename} does not exist, concatenating from multiple files')\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for year in years:\n",
    "            start_date = f'{year}-01-01'\n",
    "            end_date = f'{year}-12-31'\n",
    "\n",
    "            df_temp = run_querry_day_ahead_prices(country_code, start_date, end_date)\n",
    "\n",
    "            df = pd.concat([df, df_temp])\n",
    "\n",
    "        df.columns = ['Price']\n",
    "        df['Datetime'] = df.index\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        #order by datetime\n",
    "        df = df.sort_values(by='Datetime')\n",
    "        df = df.set_index('Datetime')\n",
    "        #add 1 hour to the datetime\n",
    "        df.index = df.index + pd.DateOffset(hours=1)\n",
    "        df.to_csv(base_df_filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_15min_to_hourly(df):\n",
    "    df['Date'] = df.index.floor('h')\n",
    "    df['Load_avg'] = df.groupby('Date')['Actual Load'].transform('mean')\n",
    "    df2 = df[['Date', 'Load_avg']]\n",
    "    df2 = df2.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_load_avg(country_code, base_load_df_filename=base_load_df_filename, years=years):\n",
    "\n",
    "    if os.path.exists(base_load_df_filename):\n",
    "        print(f'{base_load_df_filename} exists, reading from file')\n",
    "        df = pd.read_csv(base_load_df_filename, index_col=0, parse_dates=True)\n",
    "    else:\n",
    "        print(f'{base_load_df_filename} does not exist, concatenating from multiple files')\n",
    "        df = pd.DataFrame()\n",
    "        for year in years:\n",
    "            start_date = f'{year}-01-01'\n",
    "            end_date = f'{year}-12-31'\n",
    "\n",
    "            df_temp = run_querry_load(country_code, start_date, end_date)\n",
    "\n",
    "            df = pd.concat([df, df_temp])\n",
    "\n",
    "        #df.columns = ['Load']\n",
    "        df['Datetime'] = df.index\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'], utc=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        #order by datetime\n",
    "        df = df.sort_values(by='Datetime')\n",
    "        df = df.set_index('Datetime')\n",
    "        #add 1 hour to the datetime\n",
    "        df.index = df.index + pd.DateOffset(hours=1)\n",
    "        df = load_15min_to_hourly(df)\n",
    "        df = df[['Date', 'Load_avg']].drop_duplicates()\n",
    "        df.drop(columns=['Date'], inplace=True)\n",
    "        df.to_csv(base_load_df_filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices_loads(country_code):\n",
    "    prices_df = get_base_prices(country_code)\n",
    "    load_avg_df = get_base_load_avg(country_code)\n",
    "    df = pd.concat([prices_df, load_avg_df], axis=1)\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index': 'Datetime'})\n",
    "    df = df.set_index('Datetime')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/base_HU_2017_2023.csv does not exist, concatenating from multiple files\n",
      "./data/prices/price_2017-01-01_2017-12-31_HU.csv does not exist, downloading from ENTSO-E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\proha\\AppData\\Local\\Temp\\ipykernel_14816\\3373990245.py:15: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df = pd.concat([df, df_temp])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/prices/price_2018-01-01_2018-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/prices/price_2019-01-01_2019-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/prices/price_2020-01-01_2020-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/prices/price_2021-01-01_2021-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/prices/price_2022-01-01_2022-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/prices/price_2023-01-01_2023-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/base_load_avgs_HU_2017_2023.csv does not exist, concatenating from multiple files\n",
      "./data/loads/load_2017-01-01_2017-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2018-01-01_2018-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2019-01-01_2019-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2020-01-01_2020-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2021-01-01_2021-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2022-01-01_2022-12-31_HU.csv does not exist, downloading from ENTSO-E\n",
      "./data/loads/load_2023-01-01_2023-12-31_HU.csv does not exist, downloading from ENTSO-E\n"
     ]
    }
   ],
   "source": [
    "df = get_prices_loads(country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Load_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00+00:00</th>\n",
       "      <td>57.25</td>\n",
       "      <td>4150.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:00:00+00:00</th>\n",
       "      <td>50.21</td>\n",
       "      <td>4032.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 02:00:00+00:00</th>\n",
       "      <td>44.04</td>\n",
       "      <td>3777.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 03:00:00+00:00</th>\n",
       "      <td>32.81</td>\n",
       "      <td>3582.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 04:00:00+00:00</th>\n",
       "      <td>28.41</td>\n",
       "      <td>3515.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 19:00:00+00:00</th>\n",
       "      <td>25.05</td>\n",
       "      <td>4947.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 20:00:00+00:00</th>\n",
       "      <td>23.33</td>\n",
       "      <td>4745.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 21:00:00+00:00</th>\n",
       "      <td>9.82</td>\n",
       "      <td>4623.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 22:00:00+00:00</th>\n",
       "      <td>10.68</td>\n",
       "      <td>4530.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 23:00:00+00:00</th>\n",
       "      <td>14.95</td>\n",
       "      <td>4425.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61342 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Price  Load_avg\n",
       "Datetime                                  \n",
       "2017-01-01 00:00:00+00:00  57.25   4150.00\n",
       "2017-01-01 01:00:00+00:00  50.21   4032.75\n",
       "2017-01-01 02:00:00+00:00  44.04   3777.25\n",
       "2017-01-01 03:00:00+00:00  32.81   3582.25\n",
       "2017-01-01 04:00:00+00:00  28.41   3515.75\n",
       "...                          ...       ...\n",
       "2023-12-31 19:00:00+00:00  25.05   4947.00\n",
       "2023-12-31 20:00:00+00:00  23.33   4745.75\n",
       "2023-12-31 21:00:00+00:00   9.82   4623.50\n",
       "2023-12-31 22:00:00+00:00  10.68   4530.25\n",
       "2023-12-31 23:00:00+00:00  14.95   4425.25\n",
       "\n",
       "[61342 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_eval(y_true, y_pred):\n",
    "    error = np.mean(abs(y_true - y_pred))\n",
    "    print(f'Base evaluation (abs error): {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_eval(y_true, y_pred, load):\n",
    "    error = np.mean(abs((y_true - y_pred)*load))\n",
    "    print(f'Weighted evaluation (abs error): {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_modeling(model, train_df, test_df, input_cols, target_col):\n",
    "    model.fit(train_df[input_cols], train_df[target_col])\n",
    "    y_pred = model.predict(test_df[input_cols])\n",
    "    y_true = test_df[target_col]\n",
    "    base_eval(y_true, y_pred)\n",
    "    weighted_eval(y_true, y_pred, test_df['Load_avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(df):\n",
    "    y_pred = df['Price'].shift(7*24)\n",
    "    df['Base_pred'] = y_pred\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base evaluation (abs error): 27.71133820506965\n",
      "Weighted evaluation (abs error): 133708.64686343912\n"
     ]
    }
   ],
   "source": [
    "df = baseline_model(df)\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-12-31'\n",
    "base_eval(df.loc[start_date:end_date, 'Price'], df.loc[start_date:end_date, 'Base_pred'])\n",
    "weighted_eval(df.loc[start_date:end_date, 'Price'], df.loc[start_date:end_date, 'Base_pred'], df.loc[start_date:end_date, 'Load_avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Ma 8as adatokkal 9ig a holnapit (a mait mind ismerem)\n",
    "- baseline1 az 1 heti adat\n",
    "- baseline2 hasonló időjárású nap\n",
    "\n",
    "Kiértékelés:\n",
    "- v1 abs hiba (hány eurót tévedtünk)\n",
    "- v2 adott órában mennyi a load (termelés / fogyasztás), hiba súlyozva a teljes fogyasztással\n",
    "\n",
    "Opciók\n",
    "- recurrent nn predictor (?)\n",
    "- gbm regressor\n",
    "-- (előzö napi adatok, napelemek termelése, román adatok, hőmérséklet..., körny ország árai)\n",
    "-- walk forward opt\n",
    "\n",
    "keretrendszer\n",
    "feture inportance alapján feature selection \n",
    "- változásuk követése !!!\n",
    "\n",
    "(talán osztrák is számít, meg kell nézni melyik számít)\n",
    "\n",
    "időjárási adatok (első körben tényadatok, nem előrejelzés) próbálkozni kell, drága lehet, kb kizárt \n",
    "\n",
    "- végén fontos és ***nem fontos*** változók listája\n",
    "\n",
    "3 fontos időjárás (régiós, a napi bontás is jó)\n",
    "- hány fok van (fűtés / hűtés)\n",
    "- besugárzás\n",
    "- szélerősség\n",
    "\n",
    "\n",
    "-Hányszor volt negatív ár - statisztika róla (Meg tudjuk-e mondani, hogy mikor lesz negatív ár)\n",
    "-- Ez is lehet célváltozó és kiértékelés\n",
    "\n",
    "- Napi egy órát kikapcsoljuk, cél: mikor legyen (mert a többi órában többet tudunk termelni)\n",
    "-- Meg lehet nézni, hogy melyik lesz a legdrágább óra\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
